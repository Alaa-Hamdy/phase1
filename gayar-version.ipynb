{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Computer Vision Project</h1>\n",
    "<h2>Phase 1</h2>\n",
    "<h3>Team 3</h3>\n",
    "<ul><li>Anas Salah</li>\n",
    "<li> Alaa Hamdy </li>\n",
    "<li>Ahmed Amr </li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import libraries and packages used</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import cv2\n",
    "import imutils\n",
    "import json\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import ops\n",
    "from operator import itemgetter\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load training images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 33402 images\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "labels = []\n",
    "true_boxes = []\n",
    "\n",
    "picsFolder_path = \"train/\"\n",
    "with open('digitStruct.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# import colored pictures\n",
    "for i in range(len(data)):\n",
    "    image = cv2.imread(picsFolder_path + data[i]['filename'])\n",
    "    images.append(image)\n",
    "    temp=[]\n",
    "    for j in range(len(data[i]['boxes'])):\n",
    "        temp.append(data[i]['boxes'][j]['label'])\n",
    "    temp = np.array(temp)\n",
    "    labels.append(temp)\n",
    "    true_boxes.append(data[i]['boxes'])\n",
    "\n",
    "# for image in os.listdir(picsFolder_path):\n",
    "#     images.append(image)\n",
    "print(\"we have\",len(images),\"images\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Resizing Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resizedImages =[]\n",
    "for image in images:\n",
    "    resizedImages.append(cv2.resize(image,(100,75)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Creating Images Copy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesCopy = []\n",
    "for resizedimage in resizedImages:\n",
    "    imagesCopy.append(resizedimage)\n",
    "# print(len(imagesCopy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Converting Original Images copy from BGR to Grayscale  </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "greyImages = [] \n",
    "# Convert BGR to grayscale:\n",
    "for resizedimage in resizedImages:\n",
    "    greyImages.append(cv2.cvtColor(resizedimage, cv2.COLOR_BGR2GRAY)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adaptive Thresholding on Grayscale Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the adaptive thresholding:\n",
    "windowSize = 31\n",
    "windowConstant = -1\n",
    "\n",
    "binaryImages = []\n",
    "\n",
    "\n",
    "# Apply the threshold:\n",
    "for greyimage in greyImages:\n",
    "    binaryImages.append(cv2.adaptiveThreshold(greyimage, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,\n",
    "                                        windowSize, windowConstant))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Perform Connected Components on the Binary Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredImages =[]\n",
    "\n",
    "# Perform Connected Components:\n",
    "for binaryimage in binaryImages:\n",
    "    componentsNumber, labeledImage, componentStats, componentCentroids = cv2.connectedComponentsWithStats(binaryimage,\n",
    "                                                                                                          connectivity=4)\n",
    "    # Set the minimum pixels for the area filter to filter connected components:\n",
    "    minArea = 20\n",
    "\n",
    "    # Get the indices/labels of the remaining components based on the area stat\n",
    "    # (skip the background component at index 0)\n",
    "    remainingComponentLabels = [i for i in range(1, componentsNumber) if componentStats[i][4] >= minArea]\n",
    "    # Filter the labeled pixels based on the remaining labels,\n",
    "    # assign pixel intensity to 255 (uint8) for the remaining pixels\n",
    "    # y3ni b3d de el mafood yfdal the largest connected components that hopefully contains the digits\n",
    "    # one problem is that background connected components may still exist\n",
    "    filteredImages.append(np.where(np.isin(labeledImage, remainingComponentLabels) == True, 255, 0).astype('uint8'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Morphological Operations - Closing on Filtered Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set kernel (structuring element) size:\n",
    "kernelSize = 3\n",
    "\n",
    "# Set operation iterations:\n",
    "opIterations = 1\n",
    "\n",
    "# Get the structuring element:\n",
    "maxKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernelSize, kernelSize))\n",
    "\n",
    "closingImages = []\n",
    "\n",
    "# Perform closing:\n",
    "for filteredimage in filteredImages:\n",
    "    closingImages.append(cv2.morphologyEx(filteredimage, cv2.MORPH_CLOSE, maxKernel, None, None, opIterations,\n",
    "                                cv2.BORDER_REFLECT101))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Opening on the Closed Images </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set kernel (structuring element) size:\n",
    "kernelSize = 3\n",
    "\n",
    "# Set operation iterations:\n",
    "opIterations = 1\n",
    "\n",
    "# Get the structuring element:\n",
    "maxKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernelSize, kernelSize))\n",
    "\n",
    "openingImages = []\n",
    "\n",
    "# Perform closing:\n",
    "for closingimage in closingImages:\n",
    "    openingImages.append(cv2.morphologyEx(closingimage, cv2.MORPH_CLOSE, maxKernel, None, None, opIterations,\n",
    "                                cv2.BORDER_REFLECT101))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Gaussian Blur on Opened Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothedImages =[]\n",
    "\n",
    "# perform smoothing\n",
    "for closingimage in closingImages:\n",
    "    smoothedImages.append(cv2.GaussianBlur(closingimage, (3, 3), 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Canny Edge Detection on Smoothed Images </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeImages =[]\n",
    "\n",
    "# perform smoothing\n",
    "for closingimage in closingImages:\n",
    "# perform canny edge detection:\n",
    "    edgeImages.append(cv2.Canny(closingimage, 100, 200))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Drawing bounding boxes on digits </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 0\n",
    "# Get each bounding box\n",
    "# Find the big contours on the filtered image:\n",
    "# for edgeimage in edgeImages:\n",
    "def draw_boxes(edgeimage, copyimage):\n",
    "    contours, hierarchy = cv2.findContours(edgeimage, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_poly = [None] * len(contours)\n",
    "    # The Bounding Rectangles will be stored here:\n",
    "    boundRect = []\n",
    "    # Alright, just look for the outer bounding boxes:\n",
    "    for i, c in enumerate(contours):\n",
    "        if hierarchy[0][i][3] == -1:\n",
    "            contours_poly[i] = cv2.approxPolyDP(c, 3, True)\n",
    "            boundRect.append(cv2.boundingRect(contours_poly[i]))\n",
    "    for i in range(len(boundRect)):\n",
    "\n",
    "        color = (0, 255, 0)\n",
    "        # filter contours according to the area of the rectangle (parameter re5em)\n",
    "        if ( int(boundRect[i][2])*int(boundRect[i][3])>100 and int(boundRect[i][2])*int(boundRect[i][3])<1000):\n",
    "            cv2.rectangle(copyimage, (int(boundRect[i][0]), int(boundRect[i][1])),\n",
    "                          (int(boundRect[i][0] + boundRect[i][2]), int(boundRect[i][1] + boundRect[i][3])), color, 2)\n",
    "    # index= index + 1\n",
    "    boxes= []\n",
    "            # Iterate through each bounding box\n",
    "    for i in range(len(boundRect)):\n",
    "        # Extract the coordinates and dimensions of the bounding box\n",
    "        x, y, w, h = boundRect[i]\n",
    "            \n",
    "        # Append the bounding box to the list in the format of a dictionary\n",
    "        boxes.append({'left': x, 'top': y, 'width': w, 'height': h})\n",
    "        \n",
    "    # Return the list of bounding boxes\n",
    "    return boxes    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou(boxA, boxB):\n",
    "    # Calculate the intersection coordinates of the two bounding boxes\n",
    "    x1 = max(boxA[0], boxB[0])\n",
    "    y1 = max(boxA[1], boxB[1])\n",
    "    x2 = min(boxA[2], boxB[2])\n",
    "    y2 = min(boxA[3], boxB[3])\n",
    "    \n",
    "    # Compute the area of intersection rectangle\n",
    "    intersection_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
    "    \n",
    "    # Compute the area of both bounding boxes\n",
    "    boxA_area = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxB_area = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    \n",
    "    # Compute the union area\n",
    "    union_area = boxA_area + boxB_area - intersection_area\n",
    "    \n",
    "    # Compute the IOU\n",
    "    iou = intersection_area / union_area\n",
    "    \n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iouPicTest(truth, predicted, threshold1=0.5):\n",
    "    ious = []\n",
    "    for i in range(len(truth)):\n",
    "        for j in range(len(predicted)):\n",
    "            truth_box = [truth[i]['left'], truth[i]['top'], truth[i]['left'] + truth[i]['width'], truth[i]['top']+truth[i]['height']]\n",
    "            predicted_box = [predicted[j]['left'], predicted[j]['top'], predicted[j]['left']+predicted[j]['width'], predicted[j]['top']+predicted[j]['height']]\n",
    "            iou = box_iou(truth_box, predicted_box)\n",
    "            if iou >= threshold1:\n",
    "                ious.append(iou)\n",
    "    acc = np.average(ious) if ious else 0\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapTest(truth, predicted, threshold=0.7):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for i in range(len(predicted)):\n",
    "        matched = False\n",
    "        for j in range(len(truth)):\n",
    "            truth_box = [truth[j]['left'], truth[j]['top'], truth[j]['left'] + truth[j]['width'], truth[j]['top'] + truth[j]['height']]\n",
    "            predicted_box = [predicted[i]['left'], predicted[i]['top'], predicted[i]['left'] + predicted[i]['width'], predicted[i]['top'] + predicted[i]['height']]\n",
    "            iou = box_iou(truth_box, predicted_box)\n",
    "            if iou >= threshold:\n",
    "                matched = True\n",
    "                break\n",
    "        if matched:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    if len(truth) > 0:\n",
    "        fn = len(truth) - tp\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1_score = 2 * precision * recall / (precision + recall)\n",
    "        return tp,precision, recall, f1_score\n",
    "    else:\n",
    "        return 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalModel(image):\n",
    "    \n",
    "    boxes = []\n",
    "\n",
    "    #convert the image to greyscale\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    #increase the contrast\n",
    "    cv2.convertScaleAbs(image, image)\n",
    "\n",
    "    #apply gaussian blur to smooth the image\n",
    "    image = cv2.GaussianBlur(image, (3, 3), 0)\n",
    "\n",
    "    #apply adaptive threshholding\n",
    "    image = cv2.adaptiveThreshold(image,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,11,1)\n",
    "\n",
    "    #apply canny edge detection\n",
    "    image = cv2.Canny(image, 150, 200, 255)\n",
    "\n",
    "    #find contours in image\n",
    "    contours, hierarchy = cv2.findContours(image, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    #get the center of the image\n",
    "    h, w = image.shape\n",
    "    center = (int(w/2), int(h/2))\n",
    "\n",
    "    #calculate the bounding rectangle of each contour and add it to a dictionary, only if it's near the center\n",
    "    for i in range(len(contours)):\n",
    "        x, y, w, h = cv2.boundingRect(contours[i])\n",
    "        if abs((x + w/2) - center[0]) < 1.5*w and abs((y + h/2) - center[1]) < 1.5*h:\n",
    "            boxes.append({'left': x, 'top': y, 'width': w, 'height': h})\n",
    "\n",
    "\n",
    "    return boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.046178607221464\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "\n",
    "for i in range(0,len(images)):\n",
    "    boxes = finalModel(images[i])\n",
    "    acc.append(iouPicTest(true_boxes[i],boxes))\n",
    "print(np.average(acc)*100)\n",
    "\n",
    "for i in range(0,30):\n",
    "    image = images[i].copy()\n",
    "    predicted_boxes = finalModel(image)\n",
    "    for i in predicted_boxes:\n",
    "        cv2.rectangle(image, (i['left'], i['top']), (i['left'] +\n",
    "                        i['width'], i['top']+i['height']), (0, 255, 0), 2)\n",
    "    cv2.imshow('image',image)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Show Images with bounding boxes </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.056151360273361\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(0,1000):\n",
    "    # plt.imshow(images[i])\n",
    "    # plt.show()\n",
    "    boxes = draw_boxes(edgeImages[i],imagesCopy[i])\n",
    "    # tp,precision, recall, f1_score = overlapTest(edgeImages[i],imagesCopy[i])\n",
    "    acc.append(iouPicTest(true_boxes[i],boxes))\n",
    "    # print (acc)\n",
    "    # cv2.imshow('image',imagesCopy[i])\n",
    "    # cv2.waitKey(0)\n",
    "print(np.average(acc)*100)\n",
    "# print(precision)\n",
    "# print(recall)\n",
    "# print(f1_score)\n",
    "# print(tp/len(true_boxes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
