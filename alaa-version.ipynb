{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Computer Vision Project</h1>\n",
    "<h2>Phase 1</h2>\n",
    "<h3>Team 3</h3>\n",
    "<ul><li>Anas Salah</li>\n",
    "<li> Alaa Hamdy </li>\n",
    "<li>Ahmed Amr </li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import libraries and packages used</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import cv2\n",
    "import imutils\n",
    "import json\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load training images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 33402 images\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "picsFolder_path = \"train/train/\"\n",
    "with open('digitStruct.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# import colored pictures\n",
    "for i in range(len(data)):\n",
    "    image = cv2.imread(picsFolder_path + data[i]['filename'])\n",
    "    images.append(image)\n",
    "    temp=[]\n",
    "    for j in range(len(data[i]['boxes'])):\n",
    "        temp.append(data[i]['boxes'][j]['label'])\n",
    "    temp = np.array(temp)\n",
    "    labels.append(temp)\n",
    "# for image in os.listdir(picsFolder_path):\n",
    "#     images.append(image)\n",
    "print(\"we have\",len(images),\"images\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Resizing Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m resizedImages \u001b[39m=\u001b[39m[]\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images:\n\u001b[0;32m      3\u001b[0m     resizedImages\u001b[39m.\u001b[39mappend(cv2\u001b[39m.\u001b[39mresize(image,(\u001b[39m100\u001b[39m,\u001b[39m75\u001b[39m)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "resizedImages =[]\n",
    "for image in images:\n",
    "    resizedImages.append(cv2.resize(image,(100,75)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Creating Images Copy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesCopy = []\n",
    "for resizedimage in resizedImages:\n",
    "    imagesCopy.append(resizedimage)\n",
    "# print(len(imagesCopy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Removing background from images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "backgroundremovedImages=[]\n",
    "\n",
    "for resizedimage in resizedImages[:10]:\n",
    "    #== Parameters\n",
    "    BLUR = 21\n",
    "    CANNY_THRESH_1 = 10\n",
    "    CANNY_THRESH_2 = 100\n",
    "    MASK_DILATE_ITER = 10\n",
    "    MASK_ERODE_ITER = 10\n",
    "    MASK_COLOR = (0.0,0.0,1.0) # In BGR format\n",
    "    grayimage = cv2.cvtColor(resizedimage,cv2.COLOR_BGR2GRAY)\n",
    "     #-- Edge detection\n",
    "    edges = cv2.Canny(grayimage, CANNY_THRESH_1, CANNY_THRESH_2)\n",
    "    edges = cv2.dilate(edges, None)\n",
    "    edges = cv2.erode(edges, None)\n",
    "     #-- Find contours in edges, sort by area\n",
    "    contour_info = []\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    for c in contours:\n",
    "        contour_info.append((\n",
    "            c,\n",
    "            cv2.isContourConvex(c),\n",
    "            cv2.contourArea(c),\n",
    "        ))\n",
    "    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)\n",
    "    for cinfo in contour_info:\n",
    "        if (len(cinfo) != 0):\n",
    "            max_contour = contour_info[0]\n",
    "            #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----\n",
    "            # Mask is black, polygon is white\n",
    "            mask = np.zeros(edges.shape)\n",
    "            cv2.fillConvexPoly(mask, max_contour[0], (255))\n",
    "            #-- Smooth mask, then blur it\n",
    "            mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)\n",
    "            mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)\n",
    "            mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)\n",
    "            mask_stack = np.dstack([mask]*3)    # Create 3-channel alpha mask\n",
    "            #-- Blend masked img into MASK_COLOR background\n",
    "            mask_stack  = mask_stack.astype('float32') / 255.0\n",
    "            masking         = resizedimage.astype('float32') / 255.0\n",
    "            masked = (mask_stack * masking) + ((1-mask_stack) * MASK_COLOR)\n",
    "            masked = (masked * 255).astype('uint8')\n",
    "            backgroundremoved.append(masked)\n",
    "        else:\n",
    "            backgroundremoved.append(resizedimage)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Converting Original Images copy from BGR to Grayscale  </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "greyImages = [] \n",
    "# Convert BGR to grayscale:\n",
    "for backgroundremovedimages in backgroundremovedImages:\n",
    "    greyImages.append(cv2.cvtColor(backgroundremovedimages, cv2.COLOR_BGR2GRAY)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adaptive Thresholding on Grayscale Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the adaptive thresholding:\n",
    "windowSize = 31\n",
    "windowConstant = -1\n",
    "\n",
    "binaryImages = []\n",
    "\n",
    "\n",
    "# Apply the threshold:\n",
    "for greyimage in greyImages:\n",
    "    binaryImages.append(cv2.adaptiveThreshold(greyimage, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,\n",
    "                                        windowSize, windowConstant))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Perform Connected Components on the Binary Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredImages =[]\n",
    "\n",
    "# Perform Connected Components:\n",
    "for binaryimage in binaryImages:\n",
    "    componentsNumber, labeledImage, componentStats, componentCentroids = cv2.connectedComponentsWithStats(binaryimage,\n",
    "                                                                                                          connectivity=4)\n",
    "    # Set the minimum pixels for the area filter to filter connected components:\n",
    "    minArea = 20\n",
    "\n",
    "    # Get the indices/labels of the remaining components based on the area stat\n",
    "    # (skip the background component at index 0)\n",
    "    remainingComponentLabels = [i for i in range(1, componentsNumber) if componentStats[i][4] >= minArea]\n",
    "    # Filter the labeled pixels based on the remaining labels,\n",
    "    # assign pixel intensity to 255 (uint8) for the remaining pixels\n",
    "    # y3ni b3d de el mafood yfdal the largest connected components that hopefully contains the digits\n",
    "    # one problem is that background connected components may still exist\n",
    "    filteredImages.append(np.where(np.isin(labeledImage, remainingComponentLabels) == True, 255, 0).astype('uint8'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Morphological Operations - Closing on Filtered Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set kernel (structuring element) size:\n",
    "kernelSize = 3\n",
    "\n",
    "# Set operation iterations:\n",
    "opIterations = 1\n",
    "\n",
    "# Get the structuring element:\n",
    "maxKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernelSize, kernelSize))\n",
    "\n",
    "closingImages = []\n",
    "\n",
    "# Perform closing:\n",
    "for filteredimage in filteredImages:\n",
    "    closingImages.append(cv2.morphologyEx(filteredimage, cv2.MORPH_CLOSE, maxKernel, None, None, opIterations,\n",
    "                                cv2.BORDER_REFLECT101))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Opening on the Closed Images </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set kernel (structuring element) size:\n",
    "kernelSize = 3\n",
    "\n",
    "# Set operation iterations:\n",
    "opIterations = 1\n",
    "\n",
    "# Get the structuring element:\n",
    "maxKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernelSize, kernelSize))\n",
    "\n",
    "openingImages = []\n",
    "\n",
    "# Perform closing:\n",
    "for closingimage in closingImages:\n",
    "    openingImages.append(cv2.morphologyEx(closingimage, cv2.MORPH_CLOSE, maxKernel, None, None, opIterations,\n",
    "                                cv2.BORDER_REFLECT101))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Gaussian Blur on Opened Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothedImages =[]\n",
    "\n",
    "# perform smoothing\n",
    "for closingimage in closingImages:\n",
    "    smoothedImages.append(cv2.GaussianBlur(closingimage, (3, 3), 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Canny Edge Detection on Smoothed Images </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeImages =[]\n",
    "\n",
    "# perform smoothing\n",
    "for closingimage in closingImages:\n",
    "# perform canny edge detection:\n",
    "    edgeImages.append(cv2.Canny(closingimage, 100, 200))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Drawing bounding boxes on digits </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "# Get each bounding box\n",
    "# Find the big contours on the filtered image:\n",
    "for edgeimage in edgeImages:\n",
    "    contours, hierarchy = cv2.findContours(edgeimage, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_poly = [None] * len(contours)\n",
    "    # The Bounding Rectangles will be stored here:\n",
    "    boundRect = []\n",
    "    # Alright, just look for the outer bounding boxes:\n",
    "    for i, c in enumerate(contours):\n",
    "        if hierarchy[0][i][3] == -1:\n",
    "            contours_poly[i] = cv2.approxPolyDP(c, 3, True)\n",
    "            boundRect.append(cv2.boundingRect(contours_poly[i]))\n",
    "       # Get the dimensions of the image\n",
    "    # height, width, channels = imagesCopy[index].shape\n",
    "\n",
    "    # Sort the bounding boxes based on their areas in descending order\n",
    "    # boundRect = sorted(boundRect, key=lambda x: x[2] * x[3], reverse=True)\n",
    "\n",
    "    # Create an empty list to store the selected bounding boxes\n",
    "    # selectedBoundRect = []\n",
    "\n",
    "    # Iterate through each bounding box\n",
    "    # for i, box in enumerate(boundRect):\n",
    "    #     # Check if the current bounding box overlaps with any of the previously selected bounding boxes\n",
    "    #     overlaps = False\n",
    "    #     for selectedBox in selectedBoundRect:\n",
    "    #         if box[0] < selectedBox[0] + selectedBox[2] and box[0] + box[2] > selectedBox[0] and \\\n",
    "    #                 box[1] < selectedBox[1] + selectedBox[3] and box[1] + box[3] > selectedBox[1]:\n",
    "    #             overlaps = True\n",
    "    #             break\n",
    "    #     # If the current bounding box doesn't overlap with any of the previously selected bounding boxes, add it to the list\n",
    "    #     if not overlaps:\n",
    "    #         selectedBoundRect.append(box)\n",
    "        # print(len(boundRect))\n",
    "    # Draw the bounding boxes on the (copied) input image:\n",
    "    for i in range(len(boundRect)):\n",
    "        color = (0, 255, 0)\n",
    "        # area= int(boundRect[i][2]) * int(boundRect[i][3])\n",
    "        # x, y, w, h = boundRect[i]\n",
    "\n",
    "        # # Calculate the center of the bounding box\n",
    "        # center_x = x + (w / 2)\n",
    "        # center_y = y + (h / 2)\n",
    "\n",
    "        # filter contours according to the area of the rectangle (parameter re5em)\n",
    "        if ( int(boundRect[i][2])*int(boundRect[i][3])>100 and int(boundRect[i][2])*int(boundRect[i][3])<1000):\n",
    "            cv2.rectangle(imagesCopy[index], (int(boundRect[i][0]), int(boundRect[i][1])),\n",
    "                          (int(boundRect[i][0] + boundRect[i][2]), int(boundRect[i][1] + boundRect[i][3])), color, 2)\n",
    "    index= index + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Show Images with bounding boxes </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    # plt.imshow(images[i])\n",
    "    # plt.show()\n",
    "    cv2.imshow('image',imagesCopy[i])\n",
    "    cv2.waitKey(0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
