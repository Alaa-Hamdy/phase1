{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Computer Vision Project</h1>\n",
    "<h2>Phase 1</h2>\n",
    "<h3>Team 3</h3>\n",
    "<ul><li>Anas Salah</li>\n",
    "<li> Alaa Hamdy </li>\n",
    "<li>Ahmed Amr </li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import libraries and packages used</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import cv2\n",
    "import imutils\n",
    "import json\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load training images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 33402 images\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "picsFolder_path = \"train/train/\"\n",
    "with open('digitStruct.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# import colored pictures\n",
    "for i in range(len(data)):\n",
    "    image = cv2.imread(picsFolder_path + data[i]['filename'])\n",
    "    images.append(image)\n",
    "    temp=[]\n",
    "    for j in range(len(data[i]['boxes'])):\n",
    "        temp.append(data[i]['boxes'][j]['label'])\n",
    "    temp = np.array(temp)\n",
    "    labels.append(temp)\n",
    "# for image in os.listdir(picsFolder_path):\n",
    "#     images.append(image)\n",
    "print(\"we have\",len(images),\"images\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Resizing Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "resizedImages =[]\n",
    "for image in images[:10]:\n",
    "    resizedImages.append(cv2.resize(image,(100,75)))\n",
    "\n",
    "print(len(resizedImages))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Creating Images Copy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "imagesCopy = []\n",
    "for resizedimage in resizedImages:\n",
    "    imagesCopy.append(resizedimage)\n",
    "\n",
    "\n",
    "print(len(imagesCopy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Removing background from images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m backgroundremovedImages\u001b[39m=\u001b[39m[]\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m resizedimage \u001b[39min\u001b[39;00m resizedImages:\n\u001b[0;32m      5\u001b[0m     \u001b[39m# Check the number of channels and bit depth of the input image\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39;49m(resizedimage)\u001b[39m.\u001b[39;49mshape \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m      7\u001b[0m         \u001b[39m# If the input image has only one channel, convert it to 3-channel format\u001b[39;00m\n\u001b[0;32m      8\u001b[0m         img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(resizedimage, cv2\u001b[39m.\u001b[39mCOLOR_GRAY2BGR)\n\u001b[0;32m      9\u001b[0m     \u001b[39melif\u001b[39;00m resizedimage\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[0;32m     10\u001b[0m         \u001b[39m# If the input image has an alpha channel, remove it\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "backgroundremovedImages=[]\n",
    "\n",
    "\n",
    "for resizedimage in resizedImages:\n",
    "    # Check the number of channels and bit depth of the input image\n",
    "    if len(resizedimage).shape == 2:\n",
    "        # If the input image has only one channel, convert it to 3-channel format\n",
    "        img = cv2.cvtColor(resizedimage, cv2.COLOR_GRAY2BGR)\n",
    "    elif resizedimage.shape[2] == 4:\n",
    "        # If the input image has an alpha channel, remove it\n",
    "        img = cv2.cvtColor(resizedimage, cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "    # Convert the input image to 8-bit, 3-channel format\n",
    "    if img.dtype != 'uint8':\n",
    "        img = cv2.convertScaleAbs(img)\n",
    "    if img.shape[2] != 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Convert the input image to the L*a*b* color space\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Split the L*a*b* image into its three channels\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    # Apply Mean Shift to the 'a' channel\n",
    "    a = cv2.pyrMeanShiftFiltering(a, sp=8, sr=16)\n",
    "\n",
    "    # Merge the L*a*b* channels back into a single image\n",
    "    lab = cv2.merge((l, a, b))\n",
    "\n",
    "    # Convert the L*a*b* image back to RGB color space\n",
    "    output = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "    backgroundremovedImages.append(output)\n",
    "\n",
    "print(len(backgroundremovedImages))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Converting Original Images copy from BGR to Grayscale  </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "greyImages = [] \n",
    "# Convert BGR to grayscale:\n",
    "for backgroundremovedimages in backgroundremovedImages:\n",
    "    greyImages.append(cv2.cvtColor(backgroundremovedimages, cv2.COLOR_BGR2GRAY)) \n",
    "\n",
    "\n",
    "print(len(greyImages))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adaptive Thresholding on Grayscale Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Set the adaptive thresholding:\n",
    "windowSize = 31\n",
    "windowConstant = -1\n",
    "\n",
    "binaryImages = []\n",
    "\n",
    "\n",
    "# Apply the threshold:\n",
    "for greyimage in greyImages:\n",
    "    binaryImages.append(cv2.adaptiveThreshold(greyimage, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,\n",
    "                                        windowSize, windowConstant))\n",
    "    \n",
    "\n",
    "print(len(binaryImages))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Perform Connected Components on the Binary Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "filteredImages =[]\n",
    "\n",
    "# Perform Connected Components:\n",
    "for binaryimage in binaryImages:\n",
    "    componentsNumber, labeledImage, componentStats, componentCentroids = cv2.connectedComponentsWithStats(binaryimage,\n",
    "                                                                                                          connectivity=4)\n",
    "    # Set the minimum pixels for the area filter to filter connected components:\n",
    "    minArea = 20\n",
    "\n",
    "    # Get the indices/labels of the remaining components based on the area stat\n",
    "    # (skip the background component at index 0)\n",
    "    remainingComponentLabels = [i for i in range(1, componentsNumber) if componentStats[i][4] >= minArea]\n",
    "    # Filter the labeled pixels based on the remaining labels,\n",
    "    # assign pixel intensity to 255 (uint8) for the remaining pixels\n",
    "    # y3ni b3d de el mafood yfdal the largest connected components that hopefully contains the digits\n",
    "    # one problem is that background connected components may still exist\n",
    "    filteredImages.append(np.where(np.isin(labeledImage, remainingComponentLabels) == True, 255, 0).astype('uint8'))\n",
    "\n",
    "    \n",
    "print(len(filteredImages))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Morphological Operations - Closing on Filtered Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Set kernel (structuring element) size:\n",
    "kernelSize = 3\n",
    "\n",
    "# Set operation iterations:\n",
    "opIterations = 1\n",
    "\n",
    "# Get the structuring element:\n",
    "maxKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernelSize, kernelSize))\n",
    "\n",
    "closingImages = []\n",
    "\n",
    "# Perform closing:\n",
    "for filteredimage in filteredImages:\n",
    "    closingImages.append(cv2.morphologyEx(filteredimage, cv2.MORPH_CLOSE, maxKernel, None, None, opIterations,\n",
    "                                cv2.BORDER_REFLECT101))\n",
    "    \n",
    "\n",
    "print(len(closingImages))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Opening on the Closed Images </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set kernel (structuring element) size:\n",
    "kernelSize = 3\n",
    "\n",
    "# Set operation iterations:\n",
    "opIterations = 1\n",
    "\n",
    "# Get the structuring element:\n",
    "maxKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernelSize, kernelSize))\n",
    "\n",
    "openingImages = []\n",
    "\n",
    "# Perform closing:\n",
    "for closingimage in closingImages:\n",
    "    openingImages.append(cv2.morphologyEx(closingimage, cv2.MORPH_CLOSE, maxKernel, None, None, opIterations,\n",
    "                                cv2.BORDER_REFLECT101))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Gaussian Blur on Opened Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothedImages =[]\n",
    "\n",
    "# perform smoothing\n",
    "for closingimage in closingImages:\n",
    "    smoothedImages.append(cv2.GaussianBlur(closingimage, (3, 3), 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Canny Edge Detection on Smoothed Images </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "edgeImages =[]\n",
    "\n",
    "# perform smoothing\n",
    "for openingimages in openingImages:\n",
    "# perform canny edge detection:\n",
    "    edgeImages.append(cv2.Canny(closingimage, 100, 200))\n",
    "\n",
    "\n",
    "print(len(edgeImages))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Drawing bounding boxes on digits </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "# Get each bounding box\n",
    "# Find the big contours on the filtered image:\n",
    "for edgeimage in edgeImages:\n",
    "    contours, hierarchy = cv2.findContours(edgeimage, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_poly = [None] * len(contours)\n",
    "    # The Bounding Rectangles will be stored here:\n",
    "    boundRect = []\n",
    "    # Alright, just look for the outer bounding boxes:\n",
    "    for i, c in enumerate(contours):\n",
    "        if hierarchy[0][i][3] == -1:\n",
    "            contours_poly[i] = cv2.approxPolyDP(c, 3, True)\n",
    "            boundRect.append(cv2.boundingRect(contours_poly[i]))\n",
    "       # Get the dimensions of the image\n",
    "    # height, width, channels = imagesCopy[index].shape\n",
    "\n",
    "    # Sort the bounding boxes based on their areas in descending order\n",
    "    # boundRect = sorted(boundRect, key=lambda x: x[2] * x[3], reverse=True)\n",
    "\n",
    "    # Create an empty list to store the selected bounding boxes\n",
    "    # selectedBoundRect = []\n",
    "\n",
    "    # Iterate through each bounding box\n",
    "    # for i, box in enumerate(boundRect):\n",
    "    #     # Check if the current bounding box overlaps with any of the previously selected bounding boxes\n",
    "    #     overlaps = False\n",
    "    #     for selectedBox in selectedBoundRect:\n",
    "    #         if box[0] < selectedBox[0] + selectedBox[2] and box[0] + box[2] > selectedBox[0] and \\\n",
    "    #                 box[1] < selectedBox[1] + selectedBox[3] and box[1] + box[3] > selectedBox[1]:\n",
    "    #             overlaps = True\n",
    "    #             break\n",
    "    #     # If the current bounding box doesn't overlap with any of the previously selected bounding boxes, add it to the list\n",
    "    #     if not overlaps:\n",
    "    #         selectedBoundRect.append(box)\n",
    "        # print(len(boundRect))\n",
    "    # Draw the bounding boxes on the (copied) input image:\n",
    "    for i in range(len(boundRect)):\n",
    "        color = (0, 255, 0)\n",
    "        # area= int(boundRect[i][2]) * int(boundRect[i][3])\n",
    "        # x, y, w, h = boundRect[i]\n",
    "\n",
    "        # # Calculate the center of the bounding box\n",
    "        # center_x = x + (w / 2)\n",
    "        # center_y = y + (h / 2)\n",
    "\n",
    "        # filter contours according to the area of the rectangle (parameter re5em)\n",
    "        if ( int(boundRect[i][2])*int(boundRect[i][3])>100 and int(boundRect[i][2])*int(boundRect[i][3])<1000):\n",
    "            cv2.rectangle(imagesCopy[index], (int(boundRect[i][0]), int(boundRect[i][1])),\n",
    "                          (int(boundRect[i][0] + boundRect[i][2]), int(boundRect[i][1] + boundRect[i][3])), color, 2)\n",
    "    index= index + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Show Images with bounding boxes </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    # plt.imshow(images[i])\n",
    "    # plt.show()\n",
    "    cv2.imshow('image',imagesCopy[i])\n",
    "    cv2.waitKey(0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
